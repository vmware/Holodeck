{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Holodeck","text":""},{"location":"#what-is-holodeck","title":"What is Holodeck?","text":"<p>Holodeck is a toolkit designed to provide a standardized and automated method to deploy nested VMware Cloud Foundation (VCF) environments on a VMware ESX host or a vSphere cluster. These environments are ideal for technical capability testing by multiple teams inside a data center to explore hands-on exercises showcasing VCF capabilities to deliver a customer managed VMware Private Cloud. Holodeck is only to be used for a testing and training environment; it is ideal for anyone wanting to gain a better understanding of how VCF functions across many use cases and capabilities. Currently, there are two different versions of the Holodeck supported - Holodeck 5.2x supporting VCF 5.2.x and Holodeck 9.0 supporting VCF 5.2.x and VCF 9.0.x. </p> <p>This documentation solely focuses on Holodeck 9.0. If you need details on the previous version of Holodeck, please refer to this documentation</p>"},{"location":"#advantages-of-holodeck","title":"Advantages of Holodeck","text":"<p>While there are multiple ways to deploy nested VCF environments, this can be time consuming and may require specific settings to ensure optimal experience. That's where Holodeck comes in. Some of the challenges Holodeck helps overcome are:</p> <ul> <li> <p>Reduced hardware requirements: When operating in a physical environment, VCF requires four vSAN Ready Nodes for the management domain, and additional hosts for adding clusters or workload domains. In a nested environment, this same four to eight hosts are easily virtualized to run on a single ESX host or a vSphere cluster.</p> </li> <li> <p>Self-contained services: Holodeck comes with built-in common infrastructure services, such as NTP, DNS, AD, Certificate Services and DHCP within the environment, removing the need to rely on data center provided services during testing. Each environment needs a single external IP.</p> </li> <li> <p>Isolated networking: Holodeck removes the need for VLAN and BGP connections in the customer network early in the testing phase.</p> </li> <li> <p>Isolation between environments: Each Holodeck deployment is completely self-contained. This avoids conflicts with existing network configurations and allows for the deployment of multiple nested environments with no concerns for overlap.</p> </li> <li> <p>Multiple VCF deployments on a single VMware ESX host of sufficient capacity: A typical VCF Standard Architecture deployment of four node management domain and three node VI workload domain requires approximately 24 CPU cores, 325GB memory and 1.1TB disk for VCF 9.0.</p> </li> <li> <p>Automation and repeatability: The deployment of a nested VCF environments is almost completely hands-off, and easily repeatable.</p> </li> </ul>"},{"location":"#holodeck-environment-overview","title":"Holodeck Environment Overview","text":"<p>Each Holodeck environment contains:</p> VCF 9.0VCF 5.2 <ul> <li>A Holorouter appliance (photon OS based) with built-in networking services such as DNS, DHCP, NTP, Proxy, dynamic routing (BGP), L2 switching and optional webtop (virtual desktop) capability</li> <li>Support for VCF and VVF deployments</li> <li>vSAN ESA and OSA support</li> <li>Support for online and offline depot with proxy for VCF Installer</li> <li>Management Domain deployed with 4 nested hosts deployed as vSAN ready nodes including VCF Installer, VMware vCenter, VMware NSX, VCF Operations, VMware SDDC Manager, VCF Automation (optional)</li> <li>Optional Workload Domain deployed with 3 nested hosts deployed as vSAN ready nodes including VMware vCenter, VMware NSX and Supervisor (optional)</li> <li>Optional NSX Edge Cluster deployment in management and/or workload domain</li> <li>Deploy one or many additional 3-node vSphere cluster in management domain</li> <li>Support for provision-only mode (deploy VCF Installer and ESX hosts to allow greenfield deployment experience)</li> <li>Custom CIDR support for Holodeck network</li> <li>Custom VLAN support for Holodeck network</li> <li>Custom DNS Domain for Holodeck environment</li> </ul> <ul> <li>A Holorouter appliance (photon OS based) with built-in networking services such as DNS, DHCP, NTP, Proxy, dynamic routing (BGP), L2 switching and optional webtop (virtual desktop) capability</li> <li>Support for VCF deployment only</li> <li>vSAN OSA support only</li> <li>Management Domain deployed with 4 nested hosts deployed as vSAN ready nodes including VMware Cloud Builder, VMware vCenter, VMware NSX, VMware SDDC-Manager</li> <li>Optional Workload Domain deployed with 3 nested hosts deployed as vSAN ready nodes including VMware vCenter and VMware NSX</li> <li>Optional NSX Edge Cluster deployment in management and/or workload domain</li> <li>Deploy one or many additional 3-node vSphere cluster in management domain</li> <li>Custom CIDR support for Holodeck network</li> <li>Custom VLAN support for Holodeck network</li> <li>Custom DNS Domain for Holodeck environment</li> </ul> <p>Note: Holodeck 9.0 is not a VMware supported product, it is similar to a Fling.</p> <p>Holodeck 9.0 supports nested VCF deployment for versions 5.2 and 9.0. This can be deployed either on a single stand-alone ESX host or a vSphere cluster based on resource availability. Please check the Pre-requisites section</p> <p>Holodeck 9.0 Support Status</p> <p>Holodeck 9.0 is not a VMware supported product; it is similar to a Fling. It is intended for testing and training environments.</p> <p>Holodeck 9.0 has been developed using PowerShell and VMware PowerCLI. We have bundled and packaged everything needed into a powershell module called Holodeck. This powershell module is provided to you as an in-built functionality within the OVA we ship called Holorouter. </p> <p>Each Holodeck environment runs an identical nested configuration. A Holodeck environment can be deployed as a Single or Dual site Configuration. Separation of the environments and between sites within an environment is handled at the  VMware vSphere Standard Switch (vSS) or VMware vSphere Distributed Switch (vDS) level. Each Holodeck pod is configured with a unique port group on the vSS/vDS per site. A VMware vSphere Port Group is configured on each vSS/vDS and configured as a VLAN trunk to facilitate communication. Components on the port group use VLAN tagging to isolate communications between nested VLANs. This removes the need to have physical VLANs plumbed to the ESX host to support nested labs. There is also an option to use an NSX overlay segment instead of a vSS/vDS port group if available.</p>"},{"location":"#holorouter-overview","title":"Holorouter Overview","text":"<p>HoloRouter is an appliance that serves as the infrastructure backbone for Holodeck. It provides infrastructure services such as Layer-3 routing, Firewall, DHCP, DNS, NTP, BGP, Proxy, Job scheduling, etc. Through these services, HoloRouter connects the nested VCF environment to the external networks. It also provides inter-connectivity between different networks in the nested VCF environment. If you are not using custom VLANs for Holodeck, then for Site-a, VLANs 0, 10 through 25 and for Site-b, VLANs 40 through 58 are used. It is equipped with a built-in webtop (Desktop UI) which allows users access to HoloRouter via a GUI. Through the webtop service, users get easy GUI access to the nested VCF environment. </p> <p>Scope of Services: - DNS: local to Site-a and Site-b of nested VCF environment, acts as forwarder - DHCP: local to Site-a and Site-b of nested VCF environment - NTP: local to Site-a and Site-b of nested VCF environment - L3 routing between VLANs of Site-a and Site-b of nested VCF environment - Firewall to control traffic to and from external networks and between the networks in the nested VCF environment - BGP: to establish relationship with NSX Tier-0 gateway for outbound connectivity for overlay networks - Proxy: allows users to control the outbound connectivity for the nested VCF environment - Job scheduling: allows users to schedule commands/scripts to be run recursively - Webtop: allows users to access HoloRouter and nested VCF environment via a simple GUI - Powershell with VMware PowerCLI and other associated modules: allows users to consume Holodeck  - All required packages to deploy and operate Holodeck</p>"},{"location":"#concepts","title":"Concepts","text":"<ul> <li> <p>Centralized Configuration: Holodeck 9.0 has been designed around the concept of a centralized config file that acts as the source of truth for nested VCF deployments. The config file is a JSON file with a set of templates that are needed to run Holodeck. Customers are not expected to interact with the config file directly or edit it. We have built powershell cmdlets that help create, edit or import the config file as needed. The default template for config file is stored in /holodeck-runtime/templates/config.json. This config.json file is replicated and placed in /holodeck-runtime/config/&lt;config-ID&gt;.json when a new holodeck config is created using the cmdlet New-HoloDeckConfig.</p> </li> <li> <p>Idempotency: We know that deploying an entire full stack SDDC deployment can be time consuming. We also know that this time can increase even further when performing nested deployments. We've all been in a situation where we reach towards the end of the deployment only to realize we missed something minor that causes deployment failure and we have to start all over again. To solve this challenge, we've brought in the idempotency feature in Holodeck 9.0. We store the state of the holodeck deployment on Holorouter thus allowing users to run the same command used to deploy Holodeck and pick up right where the code failed, eliminating the need to restart entire deployment or proceed manually in case of failure.</p> </li> <li> <p>Automated Networking: Assigning VLANs, IP addresses, routes etc for each of your deployments can seem like a daunting task. We take this pain away in Holodeck 9.0. We use a default CIDR (10.1.0.0/20) and build out the entire networking including DNS mapping for each of your nested hosts and VCF components, entire routing including BGP setup for NSX Edge peering. For end users looking to deploy Holodeck in a custom CIDR, we provide the option to bring in your own CIDR of /20 size as an input parameter and we automatically use that to deploy VCF in the CIDR you provide. End users also get an option to specify their own VLANs and DNS domain for the Holodeck environment. Holodeck uses vcf.lab as the default DNS domain but users can specify a custom DNS domain during deployment. </p> </li> <li> <p>Built-In PreChecks: Holodeck 9.0 runs a set of pre-checks when a new deployment is run to ensure everything needed is available such as all the required binaries are available in the right location or not, is the target host reachable etc.</p> </li> </ul>"},{"location":"#holodeck-networking","title":"Holodeck Networking","text":"<p>Let's take a look at the default VLANs used within the Holodeck Network for Site A:</p> <p></p>"},{"location":"#download-the-required-software","title":"Download the Required Software","text":"<p>Navigate to the Downloads Page to download Holodeck binaries.</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":""},{"location":"#physical-host-requirements","title":"Physical Host requirements","text":"VCF 5.2 Single Site Dual Site CPU 16 32 Memory 384 GB 1TB Disk 2 TB 4TB <p>If deploying VCF Automation with vSAN ESA:</p> VCF 9.0 Single Site Dual Site CPU 32 64 Memory 325 GB 768 GB Disk 1.1 TB 2.5TB <p>If deploying VCF Automation with vSAN OSA:</p> VCF 9.0 Single Site Dual Site CPU 24 48 Memory 325 GB 768 GB Disk 1.1 TB 2.5TB VVF 9.0 Single Site Dual Site CPU 12 24 Memory 256 GB 512 GB Disk 1 TB 2TB"},{"location":"#configuration-requirements","title":"Configuration requirements","text":"<ol> <li>         Create a dedicated trunk port on the vSwitch (vSS) or vDS for connecting to Holorouter. Dedicated port group ensures Holodeck does not interfere with your environment's networking. An NSX overlay trunk port group can be used instead as well.     </li> <li>         If vSS/vDS port group is used, enable security settings on the trunk port group as below: Figure: Security Settings for vSS Port Group Figure: Security Settings for vDS Port Group </li> <li>         If NSX port group is used, ensure the type is Overlay and allow VLANs 0 to 4094 (or if using default VLANs, at a minimum VLANs 0,10-25 for Site A and 40-58 for Site-B; if using custom VLANs, VLAN 0 and custom VLAN range). Create custom segment profiles with settings as per below by navigating to Networking --&gt; Segments tab on the left navigation bar, then click on Profiles tab on the right, click on Add segment profile and select the profiles as per below          Figure: IP Discovery Profile in NSX Figure: MAC Discovery Profile in NSX Figure: Segment Security Profile in NSX          Once the profiles have been created, navigate to the overlay segment you wish to use and edit the segment and update the segment profiles association.      </li> <li>         If a vCenter is used as the target for deploying nested VCF lab, then VLANs 0, 10 through 25 and 40 through 58 (or VLAN 0 and custom VLAN range as specified by the user) need to be allowed on the physical switches to allow inter-host communication within the vSphere cluster where the nested VCF deployment will occur.     </li> </ol>"},{"location":"#target-host-configuration","title":"Target Host Configuration","text":"<ul> <li>Version vSphere 8.0u3 and 9.0 have been tested and are supported</li> <li>Stand-alone non vCenter Server managed host or a vSphere cluster managed by a VMware vCenter server instance</li> <li>Virtual Standard switch and port groups configured per guidelines</li> <li>External/Customer networks required</li> <li>ESX host management IP (one per host)</li> <li>4 CPU, 4 GB RAM and 75 GB storage for Holorouter (Internet access is optional)</li> <li>Backing storage should be SSD/NVMe based</li> <li>Holorouter external IP address per Holodeck Environment</li> <li>NTP service needs to be enabled and an NTP server must be configured. If using vCenter as the target, then all hosts within the vCenter cluster must have NTP running and configured.</li> </ul>"},{"location":"#licensing","title":"Licensing","text":"<p>Holodeck 9.0 only supports VCF 5.2.x and 9.0.x in \"License Later\" deployment mode. This mode enables all functionality for 90 days from the date of  install for VCF 9.0 and for 60 days for VCF 5.2. After that time period expires, the environment will need to be redeployed, or license must be added. Licensing is the responsibility of the end-user to ensure they procure the appropriate licenses by working with their account teams.</p>"},{"location":"#deployment","title":"Deployment","text":""},{"location":"#prepare-physical-esx-for-holodeck-networking","title":"Prepare Physical ESX for Holodeck Networking","text":"<p>Each Holodeck environment requires an isolated (no uplinks) vSphere Standard Switch and corresponding Port Groups.</p>"},{"location":"#pre-requisites_1","title":"Pre-Requisites","text":"<p>External facing Port Group configured with an IP address available for each Holodeck environment to be deployed on this host.</p>"},{"location":"#esx-host-networking-configuration","title":"ESX Host Networking Configuration","text":"<p>This task describes the process for configuring a vSwitch called Holo-PG-A and a port group called Holo-PG-A</p> <p>Note: Adding the second switch and port group for Site-2 is recommended even if you do not initially deploy the second site within the pod.</p>"},{"location":"#configure-vsphere-standard-switches-for-nested-networking","title":"Configure vSphere Standard Switches for Nested Networking","text":"<ol> <li>Create a standard switch called Holo-PG-A and MTU 8000.</li> <li>Remove the uplink by clicking on the X on the uplink.</li> <li>Verify the settings and click Add</li> </ol>"},{"location":"#configure-holodeck-port-groups","title":"Configure Holodeck Port Groups","text":"<ol> <li>Add a new Port Group</li> <li>Name the Port Group Holo-PG-A</li> <li>Set VLAN ID to 4095</li> <li>Set virtual switch to Holo-PG-A</li> <li>Open security and set all to accept</li> <li>Click Add</li> </ol>"},{"location":"#deploy-holorouter-ova","title":"Deploy Holorouter OVA","text":"<p>Holodeck 9.0 supports deployments on both stand-alone ESX hosts as well as vCenter as target. Choose the appropriate tab below to follow the instructions for your specific target to deploy Holorouter.</p> Stand-Alone ESX HostvCenter <p><ol> <li>         Log in to your target ESX host web interface \"https://\"      <li>         Right Click \"Virtual Machines\" and select \"Create/Register VM\"          </li> <li>         In the modal window select \"Deploy a virtual machine from OVF or OVA file\"          </li> <li>         Give the VM a name and select the Holorouter OVA you downloaded previously          </li> <li>         Select the appropriate Storage and Networking where your Holodeck instance will be deployed. You will select an (External) port group for Management and then another (Trunk) for Site A and Site B - With the default Site configurations you can effectively use the same port group for both sites (They are on discrete VLANs and subnets).          </li> <li>         Agree to the EULA     </li> <li>         To use DHCP leave the boxes blank, if your DHCP does not offer a DNS server, please fill that in. The other options is to statically assign a Management IP, CIDR, Gateway and DNS that will have access to the rest of your network, this IP can be used to access Holorouter and the components that will be deployed by Holodeck.          </li> <li>         Select the checkboxes for SSH and/or Webtop          NOTE If Webtop is selected you'll have a \"light\" desktop with a browser available on port 30000 of the holorouter management IP. There is no authentication so be careful not to expose this externally, or do not select this option if you do not want it exposed. </li> <li>         Click Finish     </li> <p><ol> <li>         Log in to your vCenter server https:// <li>         Right click your cluster and select \"Deploy OVF Template\"          </li> <li>         Give the VM a name and select the Holorouter OVA you downloaded previously          </li> <li>         Select the appropriate Storage and Networking where your Holodeck instance will be deployed. You will select an (External) port group for Management and then another (Trunk) for Site A and Site B - With the default Site configurations you can effectively use the same port group for both sites (They are on discrete VLANs and subnets).          </li> <li>         To use DHCP leave the boxes blank, if your DHCP does not offer a DNS server, please fill that in. The other options is to statically assign a Management IP/CIDR/GW/DNS that will have access to the rest of your network, this IP can be used to access Holorouter and the components that will be deployed by Holodeck.          </li> <li>         Select the checkboxes for SSH and/or Webtop          NOTE If Webtop is selected you'll have a \"light\" desktop with a browser available on port 30000 of the holorouter management IP. There is no authentication so be careful not to expose this externally, or do not select this option if you do not want it exposed. </li> <li>         Click Finish     </li>"},{"location":"#accessing-holodeck-environment","title":"Accessing Holodeck Environment","text":"<p>Users access to the Holodeck environment is via the Holorouter. Access to Holorouter is available via two paths:</p> <ul> <li> <p>For UI access, open a web browser from a JumpHost or Console that has access to the external IP of Holorouter and navigate to the URL http://:30000 <li> <p>For CLI access, SSH to Holorouter using the command:</p> </li> <pre><code>ssh root@&lt;Holorouter IP&gt;\n</code></pre> <p>Use the password that was set for Holorouter during OVA deployment.</p>"},{"location":"#stage-software-to-build-host","title":"Stage software to build host","text":"<p>Upload the binaries for VCF Installer/Cloud Builder and VMware ESX that were downloaded in the Pre-requisites section to the below folder on holorouter:</p> VCF Version Folder Path VCF 9.0.1.0 <code>/holodeck-runtime/bin/9.0.1.0/</code> VCF 9.0.0.0 <code>/holodeck-runtime/bin/9.0.0.0/</code> VCF 5.2.2 <code>/holodeck-runtime/bin/5.2.2/</code> VCF 5.2.1 <code>/holodeck-runtime/bin/5.2.1/</code> VCF 5.2 <code>/holodeck-runtime/bin/5.2/</code> <p>The files can be downloaded by accessing the Broadcom Support Portal within the webtop UI (assuming proper entitlement is available for end-user)</p> <p>Another option is to download the files locally and use scp to copy the files using the below command:</p> <p>For VCF 9.0.1.0:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/9.0.1.0/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/9.0.1.0/\n</code></pre> <p>For VCF 9.0.0.0:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/9.0.0.0/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/9.0.0.0/\n</code></pre> <p>Note</p> <p>VCF Installer 9.0.1.0 needs to be used for deploying VCF 9.0.0.0. VCF Installer 9.0.0.0 is no longer supported on Holodeck for nested VCF deployments.</p> <p>For VCF 5.2.2:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2.2/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2.2/\n</code></pre> <p>For VCF 5.2.1:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2.1/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2.1/\n</code></pre> <p>For VCF 5.2:</p> <pre><code>scp /&lt;local-path&gt;/&lt;ESX ISO File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2/\nscp /&lt;local-path&gt;/&lt;VCF Installer OVA File Name&gt; root@&lt;Holorouter-IP&gt;:/holodeck-runtime/bin/5.2/\n</code></pre>"},{"location":"#run-holodeck-deployment","title":"Run Holodeck deployment","text":"<p>Once logged in to Holorouter via SSH or webtop (access the CLI inside webtop), run the following commands:</p> <p>Open PowerShell:</p> <pre><code>pwsh\n</code></pre> <p>Use the command below to create a new Holodeck config. This command creates a config file specific for your deployment with a unique config ID and loads the config file into the $config variable. </p> <p><code>New-HoloDeckConfig -Description &lt;Description&gt; -TargetHost &lt;Target vCenter/ESX IP/FQDN&gt; -Username &lt;username&gt; -Password &lt;password&gt;</code></p> <p></p> <p>Multiple config files can be created by running the below command multiple times for different use-cases such as 1 config for VCF 5.2 deployment and another for VCF 9.0 deployment. </p> <p>To check which config is currently loaded in your powershell session, run the below command and check the config ID or description:</p> <pre><code>$config\n</code></pre> <p></p> <p>Note that $config is specific to a powershell session. If you exit powershell and open a new session, you will need to import the config using:</p> <pre><code>Get-HoloDeckConfig\n</code></pre> <p>The above command gives a list of config files available. Note the config ID for your specific deployment to use in the below command.</p> <pre><code>Import-HoloDeckConfig -ConfigId &lt;String&gt;\n</code></pre> <p></p> <p>The same procedure can be followed if you wish to switch from one config file to another as well. </p> <p>Deploy a Holodeck instance using the New-HoloDeckInstance command. This command can be operated in 4 different ways as shown below:</p> <p></p> <p>If you notice closely, some of the parameters have a square bracket around them while others do not. The ones that have square brackets around them are optional parameters. With this information, let's look at each option in the Syntax section of the above image.</p>"},{"location":"#vvf-deployment","title":"VVF Deployment","text":"<p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-VLANRangeStart &lt;Int32[]&gt;] [-DNSDomain &lt;String&gt;] -VVF [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> <p>In the first option, we see that -VVF and -Version are mandatory, showcasing this syntax is used for VVF deployment. </p> <p>Note: VVF deployment is supported only when -Version is selected as \"9.0.0.0\" and beyond. Using -Version \"5.2\" with -VVF yields no result.</p>"},{"location":"#management-domain-only-deployment","title":"Management-Domain Only Deployment","text":"<p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] -ManagementOnly [-NsxEdgeClusterMgmtDomain] [-DeployVcfAutomation] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-VLANRangeStart &lt;Int32[]&gt;] [-DNSDomain &lt;String&gt;] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> <p>In the second option, we see that -ManagementOnly and -Version is mandatory, showcasing this syntax is used to deploy a nested VCF deployment with management domain only. </p> Parameter Type Required Description Options Default Value Version String Mandatory Provide VCF version \"9.0.0.0\", \"9.0.1.0\", \"5.2\", \"5.2.1\" or \"5.2.2\" InstanceID String Optional Optional Instance ID used as a prefix before all nested VMs deployed as part of Holodeck to help users uniquely identify their instances. If Instance ID is not provided, a random Instance ID is generated and used. String CIDR String Optional VCF instance is deployed by default in the 10.1.0.0/20 CIDR. If you wish to use a custom CIDR, provide a CIDR of /20 size String of format: \"10.3.0.0/20\" \"10.1.0.0/20\" vSANMode String Optional Support for both vSAN Express Storage Architecture (ESA) and Original Storage Architecture (OSA) \"ESA\" or \"OSA\" \"OSA\" ManagementOnly Switch Mandatory Deploys a VCF instance with Management domain only NA NsxEdgeClusterMgmtDomain Switch Optional Deploys an NSX Edge Cluster in Management domain (AVN included if deploying VCF 5.2) NA DeployVcfAutomation Switch Optional Deploys VCF Automation. This is applicable only if -Version is set to \"9.0.0.0\" and beyond. VCF Automation is not deployed by default unless this switch is used. NA ProvisionOnly Switch Optional Deploys nested ESX hosts and VCF Installer/Cloud Builder and provides JSON API specs for performing VCF deployment manually NA VLANRangeStart Array of Integers Optional VCF instance is deployed by default with VLANs 0, 10 through 25 for Site a and 40 through 58 for Site b. If you wish to use a custom VLAN range, provide the start of the custom VLAN range using this paramater. You can specify it only for a single site by just specifying the integer or for dual site using an array [n,m] where n and m are the VLAN start range for Site a and Site b respectively. The VLAN specified for Site a should have at least 16 consecutive valid VLAN IDs and for site b, it should have at least 19 consecutive valid VLAN IDs. Integer of format: [100,200] [10,40] DNSDomain String Optional VCF instance is deployed by default with DNS domain vcf.lab. The users can specify a custom DNS domain using the DNSDomain parameter. String of format: demo.lab vcf.lab Site String Optional Deploy site a or b in a VCF Instance \"a\" or \"b\" \"a\" DepotType String Optional Applicable for -Version 9.0.0.0 and beyond only. Choose whether VCF Installer should use the online or offline depot to download VCF 9 components. \"Online\" or \"Offline\" \"Online\" LogLevel String Optional Set the log level you wish to view One of \"INFO\", \"DEBUG\", \"SUCCESS\", \"WARN\", \"ERROR\" \"INFO\" DeveloperMode Switch Optional Enables automated deployments using environment variables. NA"},{"location":"#full-stack-deployment","title":"Full Stack Deployment","text":"<p><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-WorkloadDomainType &lt;String&gt;] [-NsxEdgeClusterMgmtDomain] [-NsxEdgeClusterWkldDomain] [-DeployVcfAutomation] [-DeploySupervisor] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-VLANRangeStart &lt;Int32[]&gt;] [-DNSDomain &lt;String&gt;] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode] [&lt;CommonParameters&gt;]</code></p> <p>In the third option, we see that only -Version is mandatory, but it also has an optional parameter called -WorkloadDomainType showcasing this syntax is used for deploying a full stack nested VCF deployment. -WorkloadDomainType is optional as it already has a default value set. </p> Parameter Type Required Description Options Default Value Version String Mandatory Provide VCF version \"9.0.0.0\", \"9.0.1.0\", \"5.2\", \"5.2.1\" or \"5.2.2\" InstanceID String Optional Optional Instance ID used as a prefix before all nested VMs deployed as part of Holodeck to help users uniquely identify their instances. If Instance ID is not provided, a random Instance ID is generated and used. String CIDR String Optional VCF instance is deployed by default in the 10.1.0.0/20 CIDR. If you wish to use a custom CIDR, provide a CIDR of /20 size String of format: \"10.3.0.0/20\" \"10.1.0.0/20\" vSANMode String Optional Support for both vSAN Express Storage Architecture (ESA) and Original Storage Architecture (OSA) \"ESA\" or \"OSA\" \"OSA\" WorkloadDomainType String Optional Choose whether you want to share the management domain SSO with workload domain or use a separate SSO (wld.sso). \"SharedSSO\" or \"IsolatedSSO\" \"\" NsxEdgeClusterMgmtDomain Switch Optional Deploys an NSX Edge Cluster in Management domain (AVN included if deploying VCF 5.2) NA NsxEdgeClusterWkldDomain Switch Optional Deploys an NSX Edge Cluster in Workload domain (AVN included if deploying VCF 5.2) NA DeployVcfAutomation Switch Optional Deploys VCF Automation. This is applicable only if -Version is set to \"9.0.0.0\" and beyond. VCF Automation is not deployed by default unless this switch is used. NA DeploySupervisor Switch Optional Applicable only for VCF 9.0.0.0 and beyond. Deploys Supervisor in workload domain and additional networking configuration needed to activate supervisor NA ProvisionOnly Switch Optional Deploys nested ESX hosts and VCF Installer/Cloud Builder and provides JSON API specs for performing VCF deployment manually NA VLANRangeStart Array of Integers Optional VCF instance is deployed by default with VLANs 0, 10 through 25 for Site a and 40 through 58 for Site b. If you wish to use a custom VLAN range, provide the start of the custom VLAN range using this paramater. You can specify it only for a single site by just specifying the integer or for dual site using an array [n,m] where n and m are the VLAN start range for Site a and Site b respectively. The VLAN specified for Site a should have at least 16 consecutive valid VLAN IDs and for site b, it should have at least 19 consecutive valid VLAN IDs. Integer of format: [100,200] [10,40] DNSDomain String Optional VCF instance is deployed by default with DNS domain vcf.lab. The users can specify a custom DNS domain using the DNSDomain parameter. String of format: demo.lab vcf.lab Site String Optional Deploy site a or b in a VCF Instance \"a\" or \"b\" \"a\" DepotType String Optional Applicable for -Version 9.0.0.0 and beyond only. Choose whether VCF Installer should use the online or offline depot to download VCF 9 components. \"Online\" or \"Offline\" \"Online\" LogLevel String Optional Set the log level you wish to view One of \"INFO\", \"DEBUG\", \"SUCCESS\", \"WARN\", \"ERROR\" \"INFO\" DeveloperMode Switch Optional Enables automated deployments using environment variables. NA"},{"location":"#interactive-mode-for-day-2-ops","title":"Interactive Mode for Day 2 Ops","text":"<pre><code>New-HoloDeckInstance [-Interactive] [&lt;CommonParameters&gt;]\n</code></pre> <p>The last option is used for performing day 2 activities on a Holodeck instance after it has been deployed successfully. Day 2 operations include deploying an additional vSphere cluster in the Management Domain or the Workload Domain. More day 2 capabilities to be added in future based on feedback.</p>"},{"location":"#dual-site-deployment","title":"Dual Site Deployment","text":"<pre><code>New-HoloDeckNetworkConfig -Site a -MasterCIDR &lt;string&gt;\nNew-HoloDeckNetworkConfig -Site b -MasterCIDR &lt;string&gt;\nSet-HoloRouter -dualsite\nNew-HoloDeckInstance -Site a [-MasterCIDR &lt;string&gt;] [Additional Parameters]\n</code></pre> <p>Open a new tab in powershell, import the config and run </p> <pre><code>New-HoloDeckInstance -Site b [-MasterCIDR &lt;string&gt;] [Additional Parameters]\n</code></pre> <p>Note</p> <p>If you provide a custom CIDR in New-HoloDeckNetworkConfig, then the same custom CIDR needs to be provided in New-HoloDeckInstance to avoid the custom CIDR being overwritten by the default CIDRs.</p>"},{"location":"#developer-mode","title":"Developer Mode","text":"<p>The -DeveloperMode Parameter allows you to automate deployments by defining all interactive inputs as environment variables. To run this, open a powershell session on Holorouter and define the following variables:</p> <ul> <li> <p>Online Depot Configuration</p> <p>For online depot deployments:</p> <pre><code>$env:brcm_build_token = \"build\"\n$env:enable_proxy = \"y\" or \"n\"\n</code></pre> <p>If proxy is enabled, set the following:</p> <pre><code>$env:proxy_protocol = \"http\" or \"https\"\n$env:proxy_ip = \"&lt;proxy_ip_address&gt;\"\n$env:proxy_port = \"&lt;proxy_port&gt;\"\n$env:enable_proxy_auth = \"y\" or \"n\"\n</code></pre> <p>If proxy authentication is enabled, set the following:</p> <pre><code>$env:proxy_username = \"&lt;proxy_username&gt;\"\n$env:proxy_password = \"&lt;proxy_password&gt;\"\n</code></pre> </li> <li> <p>Offline Depot Configuration</p> <p>For offline depot deployments, define the following environment variables:</p> <pre><code>$env:offline_depot_ip = \"&lt;offline_depot_ip_address&gt;\"\n$env:offline_depot_port = \"&lt;offline_depot_port&gt;\"\n$env:offline_depot_username = \"&lt;offline_depot_username&gt;\"\n$env:offline_depot_password = \"&lt;offline_depot_password&gt;\"\n</code></pre> </li> <li> <p>Datastore and Network Port Group details:</p> <pre><code>$env:datastore_name = \"&lt;datastore_name&gt;\"\n$env:trunk_port_group_name = \"&lt;trunk_port_group_name&gt;\"\n</code></pre> </li> <li> <p>If vCenter is the target, set the following:</p> <pre><code>$env:cluster_name = \"&lt;cluster_name&gt;\"\n$env:dc_name = \"&lt;datacenter_name&gt;\"\n</code></pre> </li> </ul> <p>After setting the environment variables, run the New-HoloDeckConfig and New-HoloDeckInstance command as you would in a manual deployment with the parameters you require and the user inputs will automatically be captured from the environment variables.</p> <p>For Dual Site, open 2 powershell sessions and provide the required values to the same variables in both the sessions.</p> <p>Approx times for tested workflows (for 9.0.0.0):</p> Parameters Time Notes -ManagementOnly 4-5 hours Just 4 hosts for management domain -NsxEdgeClusterMgmtDomain 5-6 hours 4 hosts without and 2 node Edge Cluster -DeployVCFAuto -DeploySupervisor 12+ 4 hosts management with VCF Automation, Supervisor implies 3 node WLD, Edges and Supervisor <p>Please note that the time mentioned above is only an indication as the actual time taken depends on multiple factors such as the physical environment, networking connectivity etc.</p>"},{"location":"#during-deployment","title":"During Deployment","text":"<p>You may see \"errors\" during the deployment phase, if they are not displayed in RED text and exit the script, they are handled and you shouldn't need to worry about them.</p>"},{"location":"#online-depot-method","title":"Online Depot method","text":"<p>This is an interactive section during the pre-checks where you select the desired datastore to use for Holodeck deployment:</p> <p></p> <p>Choose your desired trunk port group to deploy the nested VMs (ESX and VCF Installer/Cloud Builder) on:</p> <p></p> <p>Pre-Checks are completed. If going through the online route, you need to provide the broadcom support site token:</p> <p></p> <p>Networking setup completed on Holorouter:</p> <p></p> <p>Deployment initiated:</p> <p></p> <p>Nested ESX hosts getting built:</p> <p></p> <p>VCF Installer being deployed:</p> <p></p> <p>VCF bundles being downloaded:</p> <p></p> <p>Management Domain deployment initiated through VCF Installer:</p> <p></p> <p>Management Domain deployment completed and Workload Domain deployment initiated:</p> <p></p> <p>Deployment completed successfully:</p> <p></p>"},{"location":"#offline-depot-method","title":"Offline Depot method","text":"<p>For setting up the offline depot, check out the Offline Depot Page.</p> <p>The procedure is similar to online depot except instead of passing the build token, customer needs to provide the details of the offline depot interactively:</p> <p></p>"},{"location":"#post-deployment","title":"Post Deployment","text":"<p>Once Holodeck is deployed, you can access the VCF components on your browser, assuming you have opted for default DNS domain (local based on your networking setup or webtop):</p> Appliance FQDN Username Password Management Domain VCF Installer or Cloud Builder https://vcfinstaller-a.site-a.vcf.lab admin@local for VCF Installer admin for Cloud Builder VMware123!VMware123! VCF Operations https://ops-a.site-a.vcf.lab/ admin or admin@local VMware123!VMware123! VCF Automation https://auto-a.site-a.vcf.lab/  Organization: system admin VMware123!VMware123! ESX https://esx-01a.site-a.vcf.lab https://esx-02a.site-a.vcf.lab https://esx-03a.site-a.vcf.lab https://esx-04a.site-a.vcf.lab root VMware123!VMware123! Management vCenter https://vc-mgmt-a.site-a.vcf.lab/ administrator@vsphere.local VMware123!VMware123! SDDC-Manager https://sddcmanager-a.site-a.vcf.lab/ administrator@vsphere.local VMware123!VMware123! Management NSX https://nsx-mgmt-a.site-a.vcf.lab admin VMware123!VMware123! Workload Domain Workload vCenter https://vc-wld01-a.site-a.vcf.lab/ administrator@wld.sso if Isolated SSO enabled administrator@vsphere.local otherwise VMware123!VMware123! Workload NSX https://nsx-wld01-a.site-a.vcf.lab/ admin VMware123!VMware123! ESX https://esx-05a.site-a.vcf.lab https://esx-06a.site-a.vcf.lab https://esx-07a.site-a.vcf.lab root VMware123!VMware123! <p>The above table has been generated for Site A. If you have deployed Site B, replace \"site-a\" in the FQDN with \"site-b\". For example, Management vCenter for Site A is https://vc-mgmt-a.site-a.vcf.lab/ and Management vCenter for Site B is https://vc-mgmt-a.site-b.vcf.lab/</p>"},{"location":"#how-to","title":"How To","text":""},{"location":"#start-and-stop-holodeck-instance","title":"Start and Stop Holodeck Instance","text":"<p>There may be situations where you have already deployed Holodeck but need the resources for another operation. In that case, we provide cmdlets to power off Holodeck and power it back on as well.</p> <p>For powering off Holodeck:</p> <pre><code>Stop-HoloDeckInstance [-InstanceID] &lt;string&gt; [-Force]\n</code></pre> <p>Stop-HoloDeckInstance asks for confirmation which can be bypassed by using the -Force parameter.</p> <p></p> <p>For powering on Holodeck:</p> <pre><code>Start-HoloDeckInstance [-InstanceID] &lt;string&gt; [-Force]\n</code></pre> <p></p>"},{"location":"#create-new-nested-esx-hosts","title":"Create new nested ESX hosts","text":"<p>You can dynamically add new ESX hosts to an existing site using the New-HoloDeckESXiNodes cmdlet.</p> <pre><code>New-HoloDeckESXiNodes -Nodes &lt;No. of Nodes&gt; -CPU &lt;No. of vCPU&gt; -MemoryInGb &lt;Memory in GB&gt; -Site &lt;'a' or 'b'&gt; -vSANMode &lt;'ESA' or 'OSA'&gt;\n</code></pre> <p></p>"},{"location":"#remove-nested-vcf-instance","title":"Remove nested VCF Instance","text":"<p>To remove a HoloDeck Instance, run:</p> <pre><code>Remove-HoloDeckInstance [-ResetHoloRouter]\n</code></pre> <p>Remove-HoloDeckInstance will delete al the nested ESX hosts and VCF Installer/Cloud Builder VM associated to a specific instance.</p> <p>-ResetHoloRouter will remove the networking configuration setup for the nested VCF instance. When you run New-HoloDeckInstance, the networking is configured again. This involves an automatic reboot of the Holorouter.</p>"},{"location":"#get-subnets-in-holodeck","title":"Get subnets in Holodeck","text":"<p>You can list all the subnets that have been configured or reserved for HoloDeck. You can also get information about specific subnets by specifying the name, VLAN ID, Subnet range, and Gateway IP.</p> <pre><code>Get-HoloDeckSubnet [-Name &lt;string&gt;] [-vlanID &lt;string&gt;] [-Subnet &lt;string&gt;] [-Gateway &lt;string&gt;] [-Site &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nFor Site 'a':\nGet-HoloDeckSubnet -Site a | ft -AutoSize\n\nFor Site 'b':\nGet-HoloDeckSubnet -Site b | ft -AutoSize\n\nGet-HoloDeckSubnet -Site a -Name Untagged-HOL\n\nGet-HoloDeckSubnet -Site b -vlanID 50\n\nGet-HoloDeckSubnet -Site a -Gateway 10.1.1.1\n\nGet-HoloDeckSubnet -Site a -Subnet 10.1.2.0/25\n</code></pre> <p></p>"},{"location":"#get-appliance-details","title":"Get appliance details","text":"<p>You can list all the IP-Hostname entries generated by the Network Manager for HoloDeck. You can also get information about specific IP-Hostname mappings by specifying the IP, hostname, and FQDN.</p> <pre><code>Get-HoloDeckAppNetwork [-Hostname &lt;string&gt;] [-IP &lt;string&gt;] [-FQDN &lt;string&gt;] [-Site &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nFor Site 'a':\nGet-HoloDeckAppNetwork -Site a\n\nFor Site 'b':\nGet-HoloDeckAppNetwork -Site b\n\nGet-HoloDeckAppNetwork -Site a -Hostname router\n\nGet-HoloDeckAppNetwork -Site a -IP 10.1.1.10\n\nGet-HoloDeckAppNetwork -Site a -FQDN esx-01a.site-a.vcf.lab\n</code></pre> <p></p>"},{"location":"#get-bgp-configuration","title":"Get BGP configuration","text":"<p>You can list the BGP configuration generated by the Network Manager for HoloDeck. </p> <pre><code>Get-HoloDeckBGPConfig [-Site &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nFor Site 'a':\nGet-HoloDeckBGPConfig -Site a\n\nFor Site 'b':\nGet-HoloDeckBGPConfig -Site b\n</code></pre> <p></p>"},{"location":"#get-dns-entries","title":"Get DNS entries","text":"<p>You can list all the DNS entries configured in the DNS service in HoloRouter. You can also get information about a specific DNS entry by specifying its IP or FQDN. </p> <pre><code>Get-HoloDeckDNSConfig [-IP &lt;string&gt;] [-FQDN &lt;string&gt;] [&lt;CommonParameters&gt;]\n\nFor e.g.:\nGet-HoloDeckDNSConfig\n\nGet-HoloDeckDNSConfig -IP 10.1.1.1\n\nGet-HoloDeckDNSConfig -FQDN esx-02a.site-a.vcf.lab \n</code></pre> <p></p>"},{"location":"#add-or-update-dns-entries","title":"Add or Update DNS entries","text":"<p>You can configure additional DNS entries to the DNS service in HoloRouter. To do that, use the Set-HoloDeckDNSConfig cmdlet. Note that you must specify the DNS entry in single quotes (''). <pre><code>Set-HoloDeckDNSConfig -DNSRecord &lt;string&gt; [&lt;CommonParameters&gt;]\n\nFor e.g., to create a DNS entry for '10.1.1.201 harbor.site-a.vcf.lab', you would run -\nSet-HoloDeckDNSConfig -DNSRecord '10.1.1.201 harbor.site-a.vcf.lab'\n</code></pre> <p></p> <p>You can also replace the DNS entries existing in the DNS service in HoloRouter. You will still use the Set-HoloDeckDNSConfig but specify different parameters. Note that the DNS entries to be searched and replaced must be specified in single quotes (''). <pre><code>Set-HoloDeckDNSConfig -SearchDNSRecord &lt;string&gt; -ReplaceDNSRecord &lt;string&gt; -Update [&lt;CommonParameters&gt;]\n\nFor e.g., to replace the DNS entry '10.1.1.201 harbor.site-a.vcf.lab' with '10.1.1.210 harbor.site-a.vcf.lab', you would run -\nSet-HoloDeckDNSConfig -SearchDNSRecord '10.1.1.201 harbor.site-a.vcf.lab' -ReplaceDNSRecord '10.1.1.210 harbor.site-a.vcf.lab' -Update\n</code></pre> <p></p>"},{"location":"#remove-dns-entries","title":"Remove DNS entries","text":"<p>You can remove the DNS entries from the DNS service in HoloRouter. To do that, use Remove-HoloDeckDNSConfig cmdlet. You must specify the DNS entry in single quotes (''). <pre><code>Remove-HoloDeckDNSConfig -DNSRecord &lt;string&gt;  [&lt;CommonParameters&gt;]\n\nFor e.g., to remove the DNS entry '10.1.1.210 harbor.site-a.vcf.lab', you would run -\nRemove-HoloDeckDNSConfig -DNSRecord '10.1.1.210 harbor.site-a.vcf.lab'\n</code></pre> <p></p>"},{"location":"#migrate-holorouter-90-to-901","title":"Migrate Holorouter 9.0 to 9.0.1","text":"<p>Please follow the steps specified in the flowchart below to deploy and use Holorouter 9.0.1 in an existing Holodeck 9.0 instance.</p> <p></p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>Refer to the Troubleshooting section on the FAQ page for more details.</p>"},{"location":"cmd_reference/","title":"Command Reference","text":""},{"location":"cmd_reference/#new-holodeckinstance","title":"New-HoloDeckInstance","text":"<p>Creates a new HoloDeck instance \u2014 a nested VMware Cloud Foundation (VCF) lab environment for testing and training purposes.</p> <pre><code>New-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-LogLevel &lt;String&gt;] [-ProvisionOnly] -VVF [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode]\n\nNew-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] -ManagementOnly [-NsxEdgeClusterMgmtDomain] [-DeployVcfAutomation] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode]\n\nNew-HoloDeckInstance -Version &lt;String&gt; [-InstanceID &lt;String&gt;] [-CIDR &lt;String[]&gt;] [-vSANMode &lt;String&gt;] [-WorkloadDomainType &lt;String&gt;] [-NsxEdgeClusterMgmtDomain] [-NsxEdgeClusterWkldDomain] [-DeployVcfAutomation] [-DeploySupervisor] [-LogLevel &lt;String&gt;] [-ProvisionOnly] [-Site &lt;String&gt;] [-DepotType &lt;String&gt;] [-DeveloperMode]\n\nNew-HoloDeckInstance [-Interactive]\n</code></pre>"},{"location":"cmd_reference/#description","title":"Description","text":"<p>Deploys a HoloDeck instance based on the provided VCF version and optional parameters to customize the environment for management/workload domain, NSX Edge, vSAN mode, and more.</p>"},{"location":"cmd_reference/#parameters","title":"Parameters","text":"Name Description Required Default <code>-Version</code> VCF version. Valid: <code>9.0.0.0</code> , <code>9.0.1.0</code> , <code>5.2</code> , <code>5.2.1</code> , <code>5.2.2</code> \u2705 <code>-InstanceID</code> Optional prefix for all nested VMs \u274c Random <code>-CIDR</code> Custom /20 CIDR block (e.g., <code>\"10.3.0.0/20\"</code>). For Dual Site, provide in the format <code>[\"10.3.0.0/20\",\"10.4.0.0/20\"]</code> \u274c <code>10.1.0.0/20</code> <code>-vSANMode</code> vSAN type: <code>\"ESA\"</code> or <code>\"OSA\"</code> \u274c <code>OSA</code> <code>-ManagementOnly</code> Deploy only Management domain \u274c <code>False</code> <code>-WorkloadDomainType</code> <code>\"SharedSSO\"</code> or <code>\"IsolatedSSO\"</code> (VCF 5.2 only) \u274c <code>SharedSSO</code> <code>-NsxEdgeClusterMgmtDomain</code> Deploy NSX Edge Cluster in management domain \u274c <code>False</code> <code>-NsxEdgeClusterWkldDomain</code> Deploy NSX Edge Cluster in workload domain \u274c <code>False</code> <code>-DeployVcfAutomation</code> Deploy VCF Automation (VCF 9.x only) \u274c <code>False</code> <code>-DeploySupervisor</code> Deploy Supervisor (VCF 9.x only) \u274c <code>False</code> <code>-Interactive</code> Launch interactive mode for Day 2 ops \u274c <code>False</code> <code>-LogLevel</code> Log verbosity: <code>\"INFO\"</code>, <code>\"DEBUG\"</code>, etc. \u274c <code>INFO</code> <code>-ProvisionOnly</code> Provision ESX &amp; CloudBuilder/VCF Installer only \u274c <code>False</code> <code>-VVF</code> Deploys a VVF instance \u274c <code>-Site</code> Site to deploy: <code>\"a\"</code> or <code>\"b\"</code> \u274c <code>a</code> <code>-DepotType</code> For VCF 9.0: <code>\"Online\"</code> or <code>\"Offline\"</code> \u274c <code>Online</code> <code>-DeveloperMode</code> Enables automated deployments using environment variables. \u274c <code>False</code>"},{"location":"cmd_reference/#examples","title":"Examples","text":"<p>Example 1</p> <p>Deploys a VVF using 9.0 version with vSAN ESA mode using default CIDR 10.1.0.0/20 and a randomly generated Instance ID <pre><code>New-HoloDeckInstance -Version 9.0.0.0 -vSANMode ESA -VVF -DepotType Online\n</code></pre></p> <p>Example 2</p> <p>Deploys a VCF 9.0 management domain with instance ID \"holo\" using a custom CIDR 10.3.0.0/20 with vSAN OSA and uses offline depot for VCF Installer. <pre><code>New-HoloDeckInstance -Version 9.0.0.0 -InstanceID holo -CIDR 10.3.0.0/20 -vSANMode OSA -ManagementOnly -NsxEdgeClusterMgmtDomain -DeployVcfAutomation -DepotType Offline\n</code></pre></p> <p>Example 3</p> <p>Deploys nested ESX hosts for management domain and VCF Installer and creates scripts available in /holodeck-runtime/specs/ folder to manually walk-through greenfield VCF deployment. <pre><code>New-HoloDeckInstance -Version 9.0.0.0 -InstanceID holo -CIDR 10.3.0.0/20 -vSANMode OSA -ManagementOnly -NsxEdgeClusterMgmtDomain -DeployVcfAutomation -ProvisionOnly\n</code></pre></p> <p>Example 4</p> <p>Deploys a VCF 9.0 full stack instance with NSX Edge cluster deployed in both management and workload domain, VCF Automation deployed in Management domain, supervisor deployed in workload domain using an online depot. <pre><code>New-HoloDeckInstance -Version 9.0.0.0 -NsxEdgeClusterMgmtDomain -NsxEdgeClusterWkldDomain -DeployVcfAutomation -DeploySupervisor -DepotType Online\n</code></pre></p> <p>Example 5</p> <p>Deploy additional cluster in management domain or workload domain after VCF instance has been deployed. <pre><code>New-HoloDeckInstance -Interactive\n</code></pre></p>"},{"location":"downloads/","title":"Downloads","text":"Downloads"},{"location":"downloads/#holodeck-901","title":"Holodeck 9.0.1","text":""},{"location":"downloads/#what-youll-need","title":"What You'll Need","text":"<p>To set up and deploy Holodeck 9.0.1, download the following components:</p>"},{"location":"downloads/#1-holorouter-ova","title":"1. HoloRouter OVA","text":"<p>Download the base appliance to run Holodeck: Download HoloRouter OVA</p> <p>Note: Ensure you're signed in to the Broadcom Support Portal with an entitled account.</p>"},{"location":"downloads/#2-vcf-binaries","title":"2. VCF Binaries","text":"<p>Download the necessary VCF installation components (ESX, Cloud Builder / VCF Installer) from Broadcom:</p> <p>Broadcom Support Portal \u2013 VMware Cloud Foundation </p> <p>Requires a Broadcom account with VCF entitlement.</p>"},{"location":"downloads/#3-offline-depot-optional","title":"3. Offline Depot (Optional)","text":"<p>For air-gapped or internet-restricted environments:</p> <p>Download the Offline Depot OVA</p>"},{"location":"downloads/#holodeck-90","title":"Holodeck 9.0","text":""},{"location":"downloads/#what-youll-need_1","title":"What You'll Need","text":"<p>To set up and deploy Holodeck 9.0, download the following components:</p>"},{"location":"downloads/#1-holorouter-ova_1","title":"1. HoloRouter OVA","text":"<p>Download the base appliance to run Holodeck: Download HoloRouter OVA</p> <p>Note: Ensure you're signed in to the Broadcom Support Portal with an entitled account.</p>"},{"location":"downloads/#2-vcf-binaries_1","title":"2. VCF Binaries","text":"<p>Download the necessary VCF installation components (ESX, Cloud Builder / VCF Installer) from Broadcom:</p> <p>Broadcom Support Portal \u2013 VMware Cloud Foundation</p> <p>Requires a Broadcom account with VCF entitlement.</p>"},{"location":"downloads/#3-offline-depot-optional_1","title":"3. Offline Depot (Optional)","text":"<p>For air-gapped or internet-restricted environments:</p> <p>Download the Offline Depot OVA</p>"},{"location":"downloads/#getting-started","title":"Getting Started","text":"<p>Once you've downloaded all components, head over to the Getting Started tab for step-by-step instructions.</p>"},{"location":"downloads/#need-help","title":"Need Help?","text":"<p>Visit the Support Page to report issues or join the VCF community for discussions.</p>"},{"location":"faq/","title":"FAQ","text":"\u2753 Frequently Asked Questions (FAQ) <p>Welcome! Here are answers to some of the most common questions about Holodeck and deploying VCF 9.0.</p>"},{"location":"faq/#getting-started","title":"Getting Started","text":"What is Holodeck? <p>Holodeck is a validated reference architecture and toolkit that simplifies the setup of VMware Cloud Foundation (VCF) lab environments. It includes pre-built components like HoloRouter, pre-configured networking, and deployment scripts to accelerate VCF evaluation and training.</p> What version of VCF is supported? <p>Holodeck 9.0.1 currently supports VCF 9.0.0.0, VCF 9.0.1.0, VCF 5.2, VCF 5.2.1 and VC 5.2.2.</p> Is internet access required for deployment? <p>Holodeck can be used in both connected and air-gapped environments. For offline deployments, you must download the Offline Depot OVA in advance.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":"What are the system requirements for running Holodeck? <p>Refer to the Pre-requisites section for details.</p> Do I need a license to use VCF 9.0? <p>VCF 9.0 provides a 90-day trial post which customers are required to apply license.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#general-issues","title":"General Issues","text":"Where can I report a bug or issue? <p>You can open an issue on GitHub: \ud83d\udd17 vmware/Holodeck \u2013 GitHub Issues</p> I can't access the Broadcom Support Portal downloads. <ul> <li>Ensure you're signed in with a Broadcom account.</li> <li>Your account must have entitlement for VCF 9.0.</li> <li>Contact Broadcom support if access problems persist.</li> </ul>"},{"location":"faq/#pre-check-issues","title":"Pre-check Issues","text":"I'm getting <code>&lt;Component&gt; binary was not found. Download the binary from the Broadcom support portal and place it in /holodeck-runtime/bin/&lt;version&gt; folder</code>. What should I do? <p>This error occurs when the ESX and VCF Installer/Cloud Builder ISO/OVA are not placed in the required folder /holodeck-runtime/bin/&lt;9.0 or 5.2&gt;/</p> <p>Ensure that the binaries are present in the folder.</p> I'm getting <code>[ERROR] No trunk port groups found on the connected server</code>. What should I do? <p>This error appears when running the Holodeck pre-checks script, and indicates that no trunk port group was found on the target host:</p> <pre><code>PreChecks[xxxxx]: [ERROR] No trunk port groups found on the connected server. Please create a trunk port group on the Target Host and run the pre-checks script again. Exiting\n</code></pre> <p>What it means: - The pre-check process is validating the ESX host's networking setup. - A required trunk port group (i.e., one that accepts VLAN ID <code>4095</code> or multiple VLANs) does not exist on the host.</p> <p>How to fix:</p> <p>If using stand-alone ESX as the target:</p> <ol> <li>Log in to the vSphere Host Client for the ESX host </li> <li>Go to Networking \u2192 Port Groups</li> <li>Create a new port group (or edit an existing one) with the following:</li> <li>VLAN ID: <code>4095</code> (this enables VLAN trunking)</li> <li>Virtual Switch: Usually <code>vSwitch0</code> or the one attached to your uplink</li> <li>Promiscuous Mode: Accept</li> <li>MAC Address Changes: Accept</li> <li>Forged Transmits: Accept</li> <li>Save the settings and rerun the pre-checks script</li> </ol> <p>If using vCenter as the target, then perform the same steps but for the vDS.</p> <p>Why this matters: Holodeck relies on a trunk port group to simulate physical networks and allow nested components (like NSX and VCF VMs) to communicate across VLANs.</p> <p>\ud83d\udee0\ufe0f Example port group config: - Name: <code>Trunk-PG</code> - VLAN ID: <code>4095</code> - Switch: <code>vSwitch0</code></p> <p>If you're unsure how to create this, refer to VMware's documentation or the Holodeck deployment guide.</p> I'm getting <code>[ERROR] NTP service is not running on target host</code>. How do I fix this? <p>During the pre-checks phase, you may encounter this error:</p> <pre><code>PreChecks[xxxxx]: [ERROR] NTP service is not running on target host 10.162.11.248. Enable NTP service before proceeding\n</code></pre> <p>What it means: - The Network Time Protocol (NTP) service on the ESX host is not active. - Accurate and synchronized time is required across all hosts in a VCF deployment to avoid certificate, authentication, and cluster issues.</p> <p>How to fix: 1. Log in to the ESX host UI (e.g., <code>https://10.162.11.248</code>) 2. Navigate to: Manage \u2192 System \u2192 Time &amp; Date 3. Click Edit Settings    - Set time synchronization to NTP    - Add a known NTP server (e.g. <code>pool.ntp.org</code> or your internal time server) 4. Save the settings 5. Start the NTP service:    - Under the Time &amp; Date tab, click Start next to the NTP service</p> <p>CLI Option (Advanced):</p> <p>SSH into the host and run:</p> <pre><code>esxcli network firewall ruleset set -e true -r ntp\nesxcli system ntp set -s pool.ntp.org\nesxcli system ntp set -e true\nesxcli system ntp start\n</code></pre> <p>After enabling NTP, rerun the pre-checks script.</p> <p>Important: All hosts should use the same time source to avoid clock drift across the VCF environment.</p>"},{"location":"faq/#esx-build-issues","title":"ESX Build Issues","text":"I see <code>SSH service on &lt;IP&gt; is not responding</code> and <code>WARNING: Host key is not being verified since Force switch is used</code> during the ESX build process. What does it mean? <p>This log output typically appears during the host validation step:</p> <pre><code>WARNING: Host key is not being verified since Force switch is used.\nCustomIso[xxxxx]: [INFO] SSH service on 10.3.1.101 is not responding, Trying after 10 seconds\n</code></pre> <p>What it means: - The deployment script is attempting to connect to the ESX host via SSH. - It\u2019s retrying because the host is still booting or SSH is not yet available. - This is expected behavior and can take around 10 minutes for the ESX to be ready.</p> <p>What to do: - Wait for 10-15 minutes \u2014 SSH usually becomes available shortly after ESX finishes booting. - If the message persists for more than 15 minutes, confirm that:   - The ESX VM is powered on and reachable   - Networking is properly configured in your Holodeck setup</p> <p>Still stuck? Try manually SSHing into the host to confirm access:</p> <pre><code>ssh root@10.3.1.101\n</code></pre> <p>If it fails, the host may not have finished booting or networking might be misconfigured.</p>"},{"location":"faq/#miscellaneous","title":"Miscellaneous","text":"I don't have enough memory on my physical server. Can I enable memory tiering and still deploy Holodeck? <p>while memory tiering is great for optimized performance and resource consumption in case of application workloads, it is not recommended when running nested environments,  it is not recommended when running nested environments like Holodeck. These environments are sensitive to latency and memory access speeds, and memory tiering can introduce  significant performance degradation.</p> <p>Although the deployment might technically complete, you may experience: - Crashes or instability in nested ESX hosts - Prolonged deployment times - Overall sluggish performance</p> <p>\ud83d\udc49 Recommendation: Only deploy Holodeck on systems with sufficient physical RAM and without memory tiering enabled to ensure a stable and responsive experience.</p> I deployed VCF in vSAN ESA mode with large disks, but the datastore appears fully consumed. Why? <p>When using vSAN ESA (Express Storage Architecture) in a nested setup, it's common to see high disk usage even when the documented minimum requirements are met.</p> <p>\ud83d\udd0d Why this happens: - ESA is designed for high-throughput, direct-attached NVMe or vSAN-backed storage. - In a nested environment, performance and storage efficiency are reduced\u2014especially if the underlying physical storage is not vSAN.</p> <p>\ud83d\udeab If you're running nested vSAN ESA on non-vSAN storage (e.g. local disks or NFS), you may encounter: - Unexpected datastore consumption - Poor performance or storage overhead - Inefficient space usage due to thin provisioning at multiple layers</p> <p>\u2705 Recommendation: For optimal results, deploy Holodeck and nested vSAN ESA on physical infrastructure that is also running vSAN.</p>"},{"location":"faq/#community-resources","title":"Community &amp; Resources","text":"Where can I get help or connect with others? <ul> <li>Join the Broadcom VCF Community: VMware Holodeck Community</li> <li>Open a GitHub discussion or issue</li> <li>Stay tuned for webinars and virtual workshops</li> </ul>"},{"location":"faq/#downloads","title":"Downloads","text":"Where can I find the download links? <p>Visit the Downloads Page for all required OVAs, installers, and optional offline tools.</p> <p>Need more help? Visit the Support Page or submit a an issue via GitHub</p>"},{"location":"offline_depot/","title":"Offline Depot","text":"Offline Depot"},{"location":"offline_depot/#overview","title":"Overview","text":"<p>The Offline Depot Appliance (ODA) facilitates the creation and maintenance of a VMware Cloud Foundation (VCF) offline depot. An offline depot stores software for VCF installation or updates.</p> <p>While VCF has built-in capabilities to use an online depot managed by Broadcom, an offline depot offers several key advantages:</p> <ul> <li>Restricted Environments: Enables VCF deployments in environments with limited or no external network connectivity. </li> <li>Faster Speeds: Provides quicker download and installation speeds due to local proximity. </li> <li>Content Curation: Allows organizations to control and curate the VCF binaries available to their instances. </li> </ul> <p>Additionally, the ODA provides features beyond basic offline depot functionality:</p> <ul> <li>Holodeck Support: Integrates with Holodeck for automated deployment of virtualized VCF instances, reducing physical hardware requirements and costs.</li> <li>Simplified Management: Optionally includes a Jupyter Lab instance with notebooks for streamlined depot maintenance and Holodeck integration tasks.</li> </ul>"},{"location":"offline_depot/#obtaining-a-download-token","title":"Obtaining a Download Token","text":"<p>In order to leverage the ODA to download and populate the depot with the required binaries, you will need a download token. </p> <p>You can obtain a download token by following the instructions described in this Broadcom KnowledgeBase Article.</p>"},{"location":"offline_depot/#deployment-options-if-using-holodeck","title":"Deployment Options if Using Holodeck","text":"<p>When using Holodeck, there are two primary options for deploying the ODA:</p> <ul> <li>Management Network Deployment (Recommended)</li> </ul> <p>Deploying the ODA on the management network is the recommended method. This configuration provides the ODA with direct network access to download binaries and enables full functionality, including the AI chatbot. Additionally, the integrated Jupyter notebooks can be leveraged to streamline tasks such as copying binaries to the Holorouter and accessing HoloDeck configuration information.</p> <ul> <li>Isolated Network Deployment</li> </ul> <p>While deploying the ODA on an isolated network is possible, it requires additional steps. Initially, the isolated network lacks external connectivity until the Holorouter is fully configured with BGP and DNS. This means binaries cannot be directly downloaded to the ODA. As a workaround, you must manually download and copy the SDDC Manager and ESX binaries to the Holorouter before initiating the Holorouter deployment. Once the Holorouter is online and configured, it can forward internet requests, enabling direct binary downloads to the ODA.</p>"},{"location":"offline_depot/#deploying-the-offline-depot-appliance","title":"Deploying the Offline Depot Appliance","text":"<ol> <li>         To deploy the appliance, simply deploy the OVA on your vCenter Server like you would normally do.          </li> <li>         Next, provide a name for the VM and specify its deployment location:          </li> <li>         Then, select the compute resource for the VM. You can choose to automatically start the VM after import by checking the box; otherwise, you can power it on manually to observe the first boot process.          </li> <li>         Review the settings and proceed by clicking Next.          </li> <li>         Next, select appropriate storage with sufficient capacity for your needs.          </li> <li>         On the next screen, select the network for ODA deployment. As discussed in the previous section, it is recommended to deploy the appliance on the management network.          </li> <li>         Now, specify the networking attributes, including hostname, IP address, and netmask, according to your environment's requirements.          </li> </ol> <p>The subsequent section allows you to set the password for the admin user, which is used for SSH access to the appliance.</p> <p>Following this, configure the depot-specific options:</p> <ul> <li> <p>Skip Binary Download:   Selecting this option prevents the appliance from automatically downloading required VCF binaries during power-on, which is useful if a download token is not available at installation. The depot can be populated manually or using the included Jupyter Notebooks after deployment.</p> </li> <li> <p>Download Token:   Provide a download token to enable automatic download of VCF binaries upon appliance power-on. If no token is provided, the download attempt will time out. Similar to skipping the download, manual population or Jupyter Notebooks can be used later.</p> </li> <li> <p>VCF Version:   Specify the target VCF version for binary downloads (e.g., '9.0' for VCF 9). The Advanced section offers options to enable SSH and the Jupyter Lab server. Enabling the Jupyter Lab server is highly recommended and is selected by default.</p> </li> </ul>"},{"location":"offline_depot/#initial-boot","title":"Initial Boot","text":"<p>After you deploy the appliance, you'll want to power it on. Note that the first time you power it on, it will perform some configuration steps and then reboot itself. These configuration steps will only occur the first time you boot the VM.</p> <p>Please wait until the second boot completes until trying to use the appliance.</p>"},{"location":"offline_depot/#accessing-the-appliance-web-server","title":"Accessing the Appliance Web Server","text":"<p>With the appliance online, use a web browser and go to:</p> <p><code>http://&lt;ODA_IP&gt;</code></p> <p>You should see this if you selected the option to skip the automatic download of the binaries or if the appliance had some issue trying to download the binaries:</p> <p>This indicates that no binaries have been put into place on the depot. After you download binaries (see below) then this will be populated.</p>"},{"location":"offline_depot/#accessing-the-jupyter-lab-server","title":"Accessing the Jupyter Lab Server","text":"<p>You can also access the Jupyter Lab server (if you enabled it) by using the following URL: </p> <p>http://:8888 <p>Here, you will see two Jupyter notebooks that can assist you in performing a variety of tasks. </p>"},{"location":"offline_depot/#accessing-the-vcf-product-documentation","title":"Accessing the VCF Product Documentation","text":"<p>To make things a bit easier for people, copies of the VCF 5.2 and 9.0 product documentation is included on the appliance under /var/www/docs</p>"},{"location":"offline_depot/#logging-into-the-oda","title":"Logging into the ODA","text":"<p>You can login to the ODA appliance as the user admin with the password you set at boot.</p> <p>If you need to become root, simply use sudo</p> <pre><code>sudo su\n</code></pre>"},{"location":"offline_depot/#populating-the-binaries","title":"Populating the binaries","text":"<p>There are two main methods you can do this by. These are listed below.</p>"},{"location":"offline_depot/#option-one-leverage-the-jupyter-notebook","title":"Option One: Leverage the Jupyter Notebook","text":"<p>This is the preferred method, as it makes things a bit easier. Simply access the Depot Maintenance Jupyter Notebook and go to the section about downloading the binaries and follow the directions.</p> <p>Please note that the Jupyter Notebook will display the output of the command cell executed (if there is any). For example, if you ran the cell to download the ESX binaries for Holodeck, you would see something similar to this:</p> <p>It's important to note that you can make these Jupyter Notebooks specific to your environment by modifying the commands or adding more.</p>"},{"location":"offline_depot/#option-two-manually-use-the-vcf-download-tool","title":"Option Two: Manually Use the VCF Download Tool","text":"<p>The VCF Download Tool (VDT) is also included with this version of the ODA under the /root/vdt/bin directory.</p> <p>The VDT tool replaces the Offline Bundle Transfer Utility (OBTU) tool previously used. It also has some extensive help that you can access by using the -help argument.</p>"},{"location":"offline_depot/#directory-structure-and-permissions","title":"Directory Structure and Permissions","text":"<p>If you populate the binaries manually, then you need to ensure the files are in the proper location with the proper permissions. </p> <p>Again, to assist you with this, you can leverage the Jupyter Notebooks or you can manually execute the commands:</p> <p>Failure to set the permissions properly will result in the inability to download the files from the depot.</p> <p>The directory structure varies depending on what version of VCF you populated the depot with. For VCF 9.0, you will see that under the root directory, you'll have a directory called PROD. Under this, you'll see a directory called COMP and here you will see all the various binaries used.</p> <p>You will also see a directory for metadata and vsan under /PROD as well. If you are populating the binaries manually, ensure these files exist.</p>"},{"location":"offline_depot/#caveats","title":"Caveats","text":"<p>Consider the following important points when using the offline depot:</p> <ul> <li>HTTPS Support: The depot does not currently support HTTPS to reduce complexity related to certificate generation and maintenance. </li> <li> <p>Workaround: You will need to modify <code>application-prod.properties</code> to allow HTTP. The VCF Installer UI may display a warning about HTTPS; this can be ignored if the necessary changes have been applied. Holodeck automates these modifications.</p> </li> <li> <p>Disk Space: The appliance is provisioned with approximately 300GB of disk space. Depending on your use case and the number of binaries, additional storage may be required. Refer to the \"How to Expand the Storage\" section for instructions on increasing disk capacity.</p> </li> <li> <p>Jupyter Lab on ESX: A known issue exists where the Jupyter Lab server may not configure correctly when the depot appliance is deployed directly to an ESX host. The depot functionality remains unaffected. </p> </li> <li>Recommendation: To avoid this issue, deploy the appliance to a vCenter instance.</li> </ul>"},{"location":"offline_depot/#how-to-expand-the-storage","title":"How to Expand the Storage","text":"<p>You might need to expand the filesystem of the depot as you populate it with more binaries. </p> <p>These procedures are documented in the included Jupyter Notebook. This allows you to execute them directly from there. However, if you chose not to install the Jupyter Notebooks, the instructions are duplicated here.</p> <p>First, you need to increase the size of the physical disk. To do this, simply edit the size of the disk for the VM and increase it to what will suit your needs. It will support up to 5.9TB in size.</p> <p>Next, we need to make sure the OS is aware of the space increase: <pre><code>echo 1 &gt; /sys/block/sda/device/rescan\n</code></pre> Finally, we will execute this command in order to resize the partition <pre><code>printf 'yes\\n100%%\\n' | parted /dev/sda resizepart 2 ---pretend-input-tty\n</code></pre> You can verify that the partition has been resized by using the following command: <pre><code>parted -s -a opt /dev/sda \"print free\"\n</code></pre></p> <p>Next, you need to expand the filesystem in order to take advantage of the new partition space by using a command like: <pre><code>resize2fs /dev/sda2\n</code></pre></p>"},{"location":"release_notes/","title":"Release Notes","text":"<p>This page documents the key features, enhancements, and capabilities available in Holodeck releases.</p>"},{"location":"release_notes/#holodeck-901-maintenance-release","title":"Holodeck 9.0.1 \u2013 Maintenance Release","text":"<p>Release Date: October 2025 Supported VCF Versions: VCF 9.0.1.0, VCF 9.0.0.0, VCF 5.2.2, VCF 5.2.1, VCF 5.2 Minimum ESX Version: 8.0 U3</p>"},{"location":"release_notes/#whats-new","title":"What's New","text":""},{"location":"release_notes/#enhancements","title":"Enhancements","text":"<ul> <li>Added support for VCF 9.0.1.0, including bypassing vSAN ESA check in VCF Installer and SDDC Manager.</li> <li>Updated CPU requirements for VCF Automation with vSAN ESA (min. 32 vCPUs) and vSAN OSA (min. 24 vCPUs).</li> <li>Added the capability to use custom VLAN ranges (sequential per site) for Holodeck and the users can specify the start of the custom VLAN range while deploying Holodeck</li> <li>Added the capability to use custom DNS domain for Holodeck by specifying it while deploying Holodeck</li> <li>Enabled vCLS Retreat Mode for 9.0 clusters by default.</li> <li>Custom vSAN HCL now in-built into Cloud Builder and VCF Installer for Dark Site deployments.</li> <li>Added user input for offline depot protocol (HTTP/HTTPS) and port flexibility.</li> <li>Implemented prechecks for proper offline depot configuration and file availability.</li> <li>Ability to select unique port group during prechecks for each site during dual site deployment.</li> <li>Reduced the boot time for HoloRouter VM</li> <li>Bumped up the resources on HoloRouter - it now uses 4 vCPUs and 8GB of memory</li> <li>Enhanced support for special characters in target host credentials.</li> <li>Improved cluster selection logic for vCenter targets with multiple folder levels.</li> <li>Added support for partial host commissioning during workload domain creation retries.</li> <li>Implemented additional guardrails for target disconnections to prevent errors.</li> <li>Improved idempotency for NSX Edge Cluster deployment retries.</li> <li>Introduced case sensitivity for input parameters to avoid issues.</li> <li>Improved error handling and cleaner code exits.</li> <li>Reduced CIDR size for VPC from /24 to /28 to free up IPs.</li> <li>Completed coverage for developer mode.</li> <li>Added error handling for unsupported targets (standalone ESX managed by vC).</li> <li>Provided default username and password values for optional offline depot authentication.</li> <li>Removed misleading \"must support https\" message from offline depot input.</li> <li>Improved idempotency for New-HolodeckNetworkConfig cmdlet</li> <li>Added access to HoloRouter's /holodeck-runtime/specs/ folder in webtop</li> <li>Moved iptables from startup_script to ip4save to avoid duplicate entries</li> <li>Configured HoloRouter to drop all inbound traffic on eth0 except on the ports with services enabled</li> <li>Improved error handling for Network Manager</li> </ul>"},{"location":"release_notes/#cmdlet-enhancements","title":"Cmdlet Enhancements","text":"<ul> <li>Multi-Version Support: Holodeck now supports VCF 5.2, 5.2.1, 5.2.2, 9.0.0.0 and 9.0.1.0. Please note that you need to upload ESX and VCF Installer (for 9.x) and Cloud Builder Appliance (for 5.2.x) versions in their respective folders at '/holodeck-runtime/bin' in holorouter.</li> <li><code>Remove-HoloDeckInstance</code>: This cmdlet has a new optional parameter to support the automated deletion of the holodeck instance. If not specified, then the instance ID will be captured from the global config file. Also, the prompt for user confirmation is removed; instead, the user will have 15 seconds to abort the operation by pressing any key on the console.</li> <li><code>New-HoloDeckConfig</code>: This cmdlet now validates the integrity of the <code>config.json</code> template. If the template file\u2019s integrity does not match, the user will see a warning before a new config file is created for the Holodeck instance. Ideally, users are not expected to modify the template config file; however, there may be edge cases where a customer chooses to do so. In such cases, the warning ensures they are informed of the change.</li> <li><code>Set-HoloDeckDNSConfig</code>: Removed the -update parameter. If the user specifies -DNSRecord, it will create a new record. If the user specifies -SearchDNSRecord and -ReplaceDNSRecord, it will update the DNS record.</li> <li>DNS Cmdlets: Removed the -ConfigPath parameter from all DNS cmdlets. It will now use the global $config, which means users should ensure HolodeckConfig is imported in the session before running DNS cmdlets</li> </ul>"},{"location":"release_notes/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>NSX Edge Cluster for Site B was trying to use BGP configuration from Site A. Fixed it to use Site B BGP configuration.</li> <li>VCF Installer for Site B was trying to get deployed in an incorrect VLAN. Fixed this.</li> <li>If NSX Edge Cluster creation task fails and another task populates in SDDC Manager, initiating retry gets stuck in an infinite loop. Fixed this to look for that specific task in SDDC Manager.</li> <li>Missing disconnection from target host was causing PowerCLI to list supervisors on the target sometimes instead of the nested vCenter. This has been fixed by adding a disconnection to target host before checking supervisor.</li> <li>Disabling vSAN HCL warning step was showcasing intermittent success and failure due to different object names for the warnings. Added error handling to ensure these errors are not thrown to the user.</li> <li>AVN deployment in 5.2 was failing due to an extra parameter being passed internally in the code. This has been fixed. AVN deployment is expected to proceed smoothly.</li> <li>Online depot connection check was failing due to incorrect API response expected in the code. This has been fixed. Online depot connection check should pass going forward.</li> <li>PreChecks was not looking at a specific cluster for ESX host version validation in case of multiple clusters when vCenter was selected as the target. Fixed this behavior.</li> <li>VVF deployment failure due to 4 host entries in spec file instead of 3. Fixed.</li> <li>Dual Site VVF data was not properly configured for Site B. Fixed.</li> <li>Dual site if executed serially was skipping over Site B deployment due to state management. Fixed.</li> <li>Single set of spec files were maintained for Dual site causing file overwrite for Site A. Fixed.</li> <li>VCF Installer for Site B needed to be on Site A Untagged-HoL network. Fixed.</li> <li>New ESX hosts would get created on retry even if the task had successfully completed in the previous try. Fixed.</li> <li>Fixed DNSMASQ configuration to not respond to AAAA queries that caused nslookup commands to time out</li> <li>Fixed the bug to point the VCF Installer in site b to use Untagged-HOL network with IP x.x.10.251</li> <li>Fixed iptables to not loose SSH and Webtop access on HoloRouter reboot</li> <li>Fixed the code to not throw an error when the network manager output file already exists</li> <li>Removed old journal logs from HoloRouter</li> <li>Fixed the issue in configuring DNSMASQ as primary DNS server in HoloRouter (during Set-HoloRouter) when HoloRouter uses DHCP</li> <li>Fixed Proxy FQDN to point to the right IP i.e. x.x.10.129 instead of x.x.1.129</li> <li>Fixed the inter-VRF communication by adding l3mdev settings on HoloRouter</li> <li>Fixed the rp_filter settings on HoloRouter</li> <li>Fixed the NTP and router IP/FQDN mappings used in VCF installer to fix the expected vs found IP warnings while deployment</li> <li>Fixed Network Manager not to fail if there are multiple files /etc/systemd/network/ folder</li> <li>Fixed Network Manager not to throw an error when the network manager output file already exists</li> </ul>"},{"location":"release_notes/#holodeck-90-initial-release","title":"Holodeck 9.0 \u2013 Initial Release","text":"<p>Release Date: June 2025 Supported VCF Versions: VCF 9.0, VCF 5.2.x Minimum ESX Version: 8.0 U3</p>"},{"location":"release_notes/#whats-new_1","title":"What's New","text":""},{"location":"release_notes/#enhancements_1","title":"Enhancements","text":"<ul> <li>Support for both VCF 9.0 and VCF 5.2 deployments from a single toolkit</li> <li>vSAN ESA and OSA deployment options</li> <li>New VVF deployment mode (VMware vSphere Foundation)</li> <li>Proxy support for online/offline depot workflows</li> <li>Enhanced PowerShell cmdlets with modular support:</li> <li><code>New-HoloDeckConfig</code>, <code>New-HoloDeckInstance</code>, <code>Start-HoloDeckInstance</code>, <code>Stop-HoloDeckInstance</code></li> <li>Option to deploy greenfield VCF environments with provision-only mode</li> </ul>"},{"location":"release_notes/#environment-breakdown","title":"Environment Breakdown","text":"<p>Each Holodeck environment includes a pre-configured set of virtual infrastructure components.</p> VCF 9.0VCF 5.2 <ul> <li>A Holorouter appliance (Photon OS based) with:</li> <li>DNS, DHCP, NTP, Proxy</li> <li>Dynamic routing (BGP), L2 switching</li> <li>Optional webtop (browser-based desktop)</li> <li>Support for VCF and VVF deployments</li> <li>Supports vSAN ESA and vSAN OSA</li> <li>Online and offline depot support for the VCF Installer</li> <li>Management Domain includes:</li> <li>4 nested ESX hosts as vSAN ready nodes</li> <li>VCF Installer, vCenter, NSX, VCF Operations, SDDC Manager</li> <li>Optional: VCF Automation</li> <li>Optional Workload Domain:</li> <li>3 nested ESX hosts</li> <li>vCenter, NSX, optional Supervisor Cluster</li> <li>Optional NSX Edge Clusters in management and/or workload domains</li> <li>Deploy additional 3-node vSphere clusters within management domain</li> <li>Provision-only mode to deploy just the installer and hosts</li> <li>Custom CIDR support for flexible networking</li> </ul> <ul> <li>A Holorouter appliance (Photon OS based) with:</li> <li>DNS, DHCP, NTP, Proxy</li> <li>Dynamic routing (BGP), L2 switching</li> <li>Optional webtop (browser-based desktop)</li> <li>Supports VCF deployment only</li> <li>vSAN OSA support</li> <li>Management Domain includes:</li> <li>4 nested ESX hosts as vSAN ready nodes</li> <li>Cloud Builder, vCenter, NSX, SDDC Manager</li> <li>Optional Workload Domain:</li> <li>3 nested ESX hosts</li> <li>vCenter and NSX</li> <li>Optional NSX Edge Clusters in management and/or workload domains</li> <li>Deploy additional 3-node vSphere clusters within management domain</li> <li>Custom CIDR support for flexible networking</li> </ul>"},{"location":"release_notes/#known-issues","title":"Known Issues","text":""},{"location":"release_notes/#common","title":"Common","text":"<ul> <li>Hosts with memory tiering enabled may cause instability in nested workloads</li> <li>vSAN ESA may consume more storage than expected due to nested deduplication/compression behavior</li> </ul>"},{"location":"release_notes/#holodeck-901","title":"Holodeck 9.0.1","text":"<p>No issues reported so far.</p>"},{"location":"release_notes/#holodeck-90","title":"Holodeck 9.0","text":"Online Depot Check Failure: <code>ConvertFrom-Json: Cannot bind argument to parameter 'InputObject' because it is null.</code> <p>If you\u2019re using the online depot route for VCF Installer configuration, there is a known issue where the final validation step fails due to a mismatch between the API response format expected by Holodeck and the actual output from VCF Installer.</p> <pre><code>27-06-2025 00:43:32 SddcMgmtDomain[63248]: [INFO] Setting up depot for VCF Installer\n27-06-2025 00:43:32 SddcMgmtDomain[63248]: [INFO] Depot Type selected: online\n27-06-2025 00:43:34 SddcMgmtDomain[63248]: [ERROR] Depot connection failed.\nConvertFrom-Json: Cannot bind argument to parameter 'InputObject' because it is null.\nException: Exiting\n</code></pre> Why did VM deployment fail with 'VM with name not found using the specified filter(s)'? <p> This issue can occur if the portgroup used by vCenter for VM placement is an uplink portgroup. Uplink portgroups are static and do not have available ports for VM deployment, which causes Holodeck to fail during the VM provisioning step.</p> <p>Symptoms: - Error: <code>VM with name 'green-esx-01a' was not found using the specified filter(s)</code> - Deployment halts with: <code>[ERROR] Deployment failed</code></p> <p>When does this happen? - Environments with multiple vSphere Distributed Switches (vDS) configured in the same vCenter. - The VM placement logic accidentally targets an uplink portgroup.</p> <p>Workaround: - Create a test portgroup in the same vDS with default settings (non-uplink, ephemeral or static binding). - Retry the Holodeck deployment process.</p> <p>Track this issue and future fix here: GitHub Issue #10</p> Users lose Webtop, SSH and ping access to Holorouter on rebooting Holorouter <p>Reason: Some of the iptables rules on HoloRouter aren't persistent across reboots </p> <p>Impacted Services: SSH, Ping, and Webtop; None of the other Holodeck infrastructure services should have any impact due to this bug.</p> <p>Workaround: </p> <ol> <li> <p>If you haven't rebooted the HoloRouter yet but are planning to, run the following command before rebooting it -  <pre><code>iptables-save &gt; /etc/systemd/scripts/ip4save\n</code></pre></p> </li> <li> <p>If you've already rebooted the HoloRouter, since you don't have SSH access, log into the HoloRouter console in the vCenter/ESX host and run the following commands - </p> </li> </ol> <p><pre><code>iptables -D INPUT -i eth0 -j DROP\niptables -A INPUT -i lo -j ACCEPT\niptables -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\niptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT\niptables -A INPUT -i eth0 -p icmp -j ACCEPT\niptables -A INPUT -i eth0 -p tcp -m tcp --dport 22 -j ACCEPT\niptables -A INPUT -i eth0 -p tcp -m tcp --dport 30000 -j ACCEPT\niptables -A INPUT -i eth0 -j DROP\n</code></pre>  Track this issue and future fix here: GitHub Issue #12</p>"},{"location":"release_notes/#workaround","title":"Workaround","text":"<ul> <li>Login to the VCF Installer UI via Webtop   (typically at <code>https://10.1.10.250</code> unless a custom CIDR was used)</li> <li>Check if the depot is already configured successfully</li> <li>If confirmed, navigate back to PowerShell and run:</li> </ul> <pre><code>vi $config.state\n</code></pre> <ul> <li>Find and manually update the section:</li> </ul> <pre><code>{\n  \"VCF-Installer-Depot-Setup\": {\n    \"Status\": \"InProgress\"\n  }\n}\n</code></pre> <ul> <li>Change <code>\"Status\": \"InProgress\"</code> to <code>\"Status\": \"Success\"</code> to unblock the process.</li> <li>Run <code>New-HoloDeckInstance</code> command again to pick up from where it failed.</li> </ul> <p>Track this issue and future fix here: GitHub Issue #1</p>"},{"location":"release_notes/#previous-versions","title":"Previous Versions","text":"<p>Looking for Holodeck 5.2 setup guidance? Refer to the Holodeck 5.2 documentation.</p>"},{"location":"release_notes/#reporting-issues","title":"Reporting Issues","text":"<p>To raise issues, feature requests, or provide feedback, please visit the Holodeck GitHub repository.</p>"},{"location":"support_community/","title":"Support","text":"Support &amp; Feedback <p>If you're encountering issues, need help, or want to engage with the community, we offer two main channels of support.</p>"},{"location":"support_community/#report-bugs-or-request-features","title":"Report Bugs or Request Features","text":"<p>For all technical issues and feature requests related to Holodeck, please use our GitHub repository:</p> <p>vmware/Holodeck \u2013 GitHub Issues</p> <p>You can: - Report bugs or unexpected behavior - Suggest new features - Ask questions about setup, usage, or troubleshooting - Recommend improvements to the documentation</p> <p>Before submitting, please check if an issue already exists.</p>"},{"location":"support_community/#join-the-broader-community","title":"Join the Broader Community","text":"<p>For wider discussions, product insights, and community-contributed content, visit the Broadcom VCF Community:</p> <p>VMware Holodeck Community</p> <p>There you\u2019ll find: - Community blogs &amp; technical articles - Product announcements - Discussions with other VCF users &amp; contributors - Tips from Broadcom and VMware teams</p>"},{"location":"support_community/#contribute-to-holodeck","title":"Contribute to Holodeck","text":"<p>Want to contribute ideas, code, or documentation to Holodeck?</p> <p>github.com/vmware/Holodeck</p> <p>Together, we can make Holodeck better for everyone</p>"}]}